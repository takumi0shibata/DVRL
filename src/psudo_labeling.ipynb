{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takumishibata/Documents/project/DVRL-AES/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import polars as pl\n",
    "\n",
    "from utils.general_utils import set_seed\n",
    "from dvrl.dataset import EssayDataset\n",
    "from models.paes import PAES\n",
    "from dvrl.predictor_config import PAESModelConfig\n",
    "from dvrl.fn_predictor import fit_func, pred_func, calc_qwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prompt_id = 1\n",
    "device = torch.device('cpu')\n",
    "set_seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading essay data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takumishibata/Documents/project/DVRL-AES/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Number of training samples: 11194\n",
      "    Number of dev samples: 30\n",
      "    Number of test samples: 1753\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "# Step1. Load Data\n",
    "###################################################\n",
    "# Load essay data\n",
    "print('Loading essay data...')\n",
    "dataset = EssayDataset('../data/training_set_rel3.xlsx', '../data/hand_crafted_v3.csv', '../data/readability_features.csv')\n",
    "dataset.preprocess_dataframe()\n",
    "train_data, dev_data, test_data = dataset.cross_prompt_split(\n",
    "    target_prompt_set=target_prompt_id,\n",
    "    dev_size=30,\n",
    "    cache_dir='.embedding_cache',\n",
    "    embedding_model='microsoft/deberta-v3-large',\n",
    "    add_pos=True,\n",
    ")\n",
    "print(f'    Number of training samples: {len(train_data[\"essay_id\"])}')\n",
    "print(f'    Number of dev samples: {len(dev_data[\"essay_id\"])}')\n",
    "print(f'    Number of test samples: {len(test_data[\"essay_id\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_index = np.array(range(len(train_data['essay_id'])))\n",
    "\n",
    "# 擬似ラベルを付与するデータとしないデータに分割\n",
    "train_index, val_index = train_test_split(\n",
    "    train_index,\n",
    "    test_size=0.2,\n",
    "    random_state=12,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "Calculating QWK...\n",
      "    Test QWK: 0.752133071144686\n"
     ]
    }
   ],
   "source": [
    "config = PAESModelConfig()\n",
    "model = PAES(train_data['max_sentnum'], train_data['max_sentlen'], train_data['pos_vocab']).to(device)\n",
    "# train data\n",
    "x_train = [train_data['pos_x'][train_index], train_data['feature'][train_index], train_data['readability'][train_index]]\n",
    "y_train = train_data['scaled_score'][train_index].reshape(-1, 1)\n",
    "# dev data\n",
    "x_dev = [train_data['pos_x'][val_index], train_data['feature'][val_index], train_data['readability'][val_index]]\n",
    "y_dev = train_data['scaled_score'][val_index].reshape(-1, 1)\n",
    "# test data\n",
    "x_test = [\n",
    "    np.concatenate([test_data['pos_x'], dev_data['pos_x']], axis=0),\n",
    "    np.concatenate([test_data['feature'], dev_data['feature']], axis=0),\n",
    "    np.concatenate([test_data['readability'], dev_data['readability']], axis=0)\n",
    "]\n",
    "y_test = np.concatenate([test_data['scaled_score'], dev_data['scaled_score']])\n",
    "\n",
    "model = fit_func(\n",
    "    model,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    config.optimizer,\n",
    "    config.lr,\n",
    "    config.batch_size,\n",
    "    config.epochs,\n",
    "    device,\n",
    "    target_prompt_id,\n",
    "    'mse',\n",
    "    x_dev,\n",
    "    y_dev,\n",
    "    False,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Predict\n",
    "print('Predicting...')\n",
    "y_dev_pred = pred_func(\n",
    "    model,\n",
    "    x_dev,\n",
    "    config.batch_size,\n",
    "    device\n",
    ")\n",
    "y_test_pred = pred_func(\n",
    "    model,\n",
    "    x_test,\n",
    "    config.batch_size,\n",
    "    device\n",
    ")\n",
    "\n",
    "# Calculate QWK\n",
    "print('Calculating QWK...')\n",
    "# dev_qwk = calc_qwk(y_dev, y_dev_pred, target_prompt_id, 'score')\n",
    "test_qwk = calc_qwk(y_test, y_test_pred, target_prompt_id, 'score')\n",
    "\n",
    "# print(f'    Dev QWK: {dev_qwk}')\n",
    "print(f'    Test QWK: {test_qwk}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "# y_test_predの値をCSVで保存\n",
    "df = pl.DataFrame({\n",
    "    'essay_id': np.concatenate([test_data['essay_id'], dev_data['essay_id']]),\n",
    "    'y_pred': y_test_pred.flatten()\n",
    "})\n",
    "df.write_csv(f'../outputs/paes/prediction_{target_prompt_id}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
