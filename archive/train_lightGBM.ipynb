{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takumishibata/.pyenv/versions/3.11.5/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from PAES.configs import PAESConfig\n",
    "from models.transfomer_enc import FeatureModel\n",
    "from dvrl.predictor_model import MLP\n",
    "from transformers import AutoConfig\n",
    "from utils.create_embedding_feautres import create_embedding_features, normalize_scores\n",
    "from utils.dvrl_utils import remove_top_p_sample, fit_func, pred_func, calc_qwk, random_remove_sample, get_dev_sample\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from utils.read_data import get_readability_features, get_linguistic_features, get_features_by_id, scale_features, get_normalized_features\n",
    "from utils.general_utils import get_min_max_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "test_prompt_id = 1\n",
    "# output_path = f'outputs/DVRL_DomainAdaptation/'\n",
    "# output_path = f'outputs/DVRL_DomainAdaptation{test_prompt_id}/'\n",
    "# output_path = f'outputs/DVRL_DomainAdaptation{test_prompt_id}_devsize0.01/'\n",
    "# output_path = f'outputs/DVRL_DomainAdaptation{test_prompt_id}_devsize30/'\n",
    "output_path = f'outputs/DVRL_DomainAdaptation_FeatureModel{test_prompt_id}_devsize40/'\n",
    "\n",
    "seed = 12\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "attribute_name = 'score'\n",
    "\n",
    "# Load data\n",
    "data_path = 'data/cross_prompt_attributes/' + str(test_prompt_id) + '/'\n",
    "model_name = 'microsoft/deberta-v3-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data from data/cross_prompt_attributes/1/...\n",
      "Loading embedding from cache...\n",
      "Selected 40 samples.\n",
      "Selected sample indices: [1691, 446, 689, 653, 945, 1548, 1244, 452, 1413, 916, 1447, 871, 1645, 476, 1368, 878, 746, 282, 1509, 1347, 151, 1577, 622, 1549, 1570, 915, 1348, 690, 794, 1359, 1299, 195, 908, 305, 1461, 410, 77, 939, 1183, 573]\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = create_embedding_features(data_path, attribute_name, model_name, device)\n",
    "# split test data into dev and test\n",
    "_, _, _, _, dev_idx, _ = get_dev_sample(test_data['essay'], test_data['normalized_label'], dev_size=40)\n",
    "\n",
    "source_ids = np.concatenate([train_data['essay_id'], val_data['essay_id']])\n",
    "dev_ids = test_data['essay_id'][dev_idx]\n",
    "target_ids = np.setdiff1d(test_data['essay_id'], dev_ids)\n",
    "\n",
    "# load readability and linguistic features\n",
    "readability_features = get_readability_features('data/allreadability.pickle')\n",
    "linguistic_features = get_linguistic_features('data/hand_crafted_v3.csv')\n",
    "\n",
    "x_source_readability = get_features_by_id(readability_features, source_ids, 'dim1').drop('dim1', axis=1).to_numpy()\n",
    "x_dev_readability = get_features_by_id(readability_features, dev_ids, 'dim1').drop('dim1', axis=1).to_numpy()\n",
    "x_target_readability = get_features_by_id(readability_features, target_ids, 'dim1').drop('dim1', axis=1).to_numpy()\n",
    "\n",
    "x_source_linguistic = get_features_by_id(linguistic_features, source_ids, 'item_id')\n",
    "x_dev_linguistic = get_features_by_id(linguistic_features, dev_ids, 'item_id')\n",
    "x_target_linguistic = get_features_by_id(linguistic_features, target_ids, 'item_id')\n",
    "\n",
    "y_source = x_source_linguistic['score'].to_numpy()\n",
    "y_source_prompt = x_source_linguistic['prompt_id'].to_numpy()\n",
    "y_source = normalize_scores(y_source, y_source_prompt, 'score')\n",
    "\n",
    "y_dev = x_dev_linguistic['score'].to_numpy()\n",
    "y_dev_prompt = x_dev_linguistic['prompt_id'].to_numpy()\n",
    "y_dev = normalize_scores(y_dev, y_dev_prompt, 'score')\n",
    "\n",
    "y_target = x_target_linguistic['score'].to_numpy()\n",
    "y_target_prompt = x_target_linguistic['prompt_id'].to_numpy()\n",
    "y_target = normalize_scores(y_target, y_target_prompt, 'score')\n",
    "\n",
    "x_source_linguistic_scaled = scale_features(x_source_linguistic).drop(['item_id', 'prompt_id', 'score'], axis=1).to_numpy()\n",
    "x_dev_linguistic_scaled = scale_features(x_dev_linguistic).drop(['item_id', 'prompt_id', 'score'], axis=1).to_numpy()\n",
    "x_target_linguistic_scaled = scale_features(x_target_linguistic).drop(['item_id', 'prompt_id', 'score'], axis=1).to_numpy()\n",
    "\n",
    "x_source = np.concatenate([x_source_readability, x_source_linguistic_scaled], axis=1)\n",
    "x_dev = np.concatenate([x_dev_readability, x_dev_linguistic_scaled], axis=1)\n",
    "x_target = np.concatenate([x_target_readability, x_target_linguistic_scaled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\n",
      "X_source:  (11193, 86)\n",
      "Y_source:  (11193,)\n",
      "================================\n",
      "X_dev:  (40, 86)\n",
      "Y_dev:  (40,)\n",
      "================================\n",
      "X_target:  (1743, 86)\n",
      "Y_target:  (1743,)\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "print('================================')\n",
    "print('X_source: ', x_source.shape)\n",
    "print('Y_source: ', y_source.shape)\n",
    "\n",
    "print('================================')\n",
    "print('X_dev: ', x_dev.shape)\n",
    "print('Y_dev: ', y_dev.shape)\n",
    "\n",
    "print('================================')\n",
    "print('X_target: ', x_target.shape)\n",
    "print('Y_target: ', y_target.shape)\n",
    "print('================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
